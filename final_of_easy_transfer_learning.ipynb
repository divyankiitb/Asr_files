{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyankiitb/Asr_files/blob/main/final_of_easy_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45ea3ef5",
      "metadata": {
        "id": "45ea3ef5"
      },
      "source": [
        "# Easy transfer learning with 🐸 STT ⚡\n",
        "\n",
        "You want to train a Coqui (🐸) STT model, but you don't have a lot of data. What do you do?\n",
        "\n",
        "The answer 💡: Grab a pre-trained model and fine-tune it to your data. This is called `\"Transfer Learning\"` ⚡\n",
        "\n",
        "🐸 STT comes with transfer learning support out-of-the box.\n",
        "\n",
        "You can even take a pre-trained model and fine-tune it to _any new language_, even if the alphabets are completely different. Likewise, you can fine-tune a model to your own data and improve performance if the language is the same.\n",
        "\n",
        "In this notebook, we will:\n",
        "\n",
        "1. Download a pre-trained English STT model.\n",
        "2. Download data for the Russian language.\n",
        "3. Fine-tune the English model to Russian language.\n",
        "4. Test the new Russian model and display its performance.\n",
        "\n",
        "So, let's jump right in!\n",
        "\n",
        "*PS - If you just want a working, off-the-shelf model, check out the [🐸 Model Zoo](https://www.coqui.ai/models)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fa2aec77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa2aec77",
        "outputId": "58be28ec-1be8-4a79-ee18-375b65113390"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-22.0.4-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 7.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-22.0.4\n",
            "Collecting coqui_stt_training\n",
            "  Downloading coqui_stt_training-1.3.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting webdataset\n",
            "  Downloading webdataset-0.2.5-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 KB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow==1.15.4\n",
            "  Downloading tensorflow-1.15.4-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optuna\n",
            "  Downloading optuna-2.10.0-py3-none-any.whl (308 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.2/308.2 KB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (1.3.5)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (3.38.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (1.21.5)\n",
            "Collecting pyogg>=0.6.14a1\n",
            "  Downloading PyOgg-0.6.14a1.tar.gz (35 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (0.10.3.post1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (4.64.0)\n",
            "Collecting opuslib==2.0.0\n",
            "  Downloading opuslib-2.0.0.tar.gz (7.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sox\n",
            "  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (0.0.1)\n",
            "Requirement already satisfied: numba<=0.53.1 in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (0.51.2)\n",
            "Collecting miniaudio\n",
            "  Downloading miniaudio-1.46-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (551 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m551.8/551.8 KB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coqui-stt-ctcdecoder==1.3.0\n",
            "  Downloading coqui_stt_ctcdecoder-1.3.0-cp37-cp37m-manylinux_2_24_x86_64.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (2.23.0)\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (2.13.0)\n",
            "Collecting coqpit\n",
            "  Downloading coqpit-0.0.15-py3-none-any.whl (13 kB)\n",
            "Collecting attrdict\n",
            "  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (1.1.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (3.17.3)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (0.37.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (1.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (3.3.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.4/503.4 KB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (1.44.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (1.14.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<=0.53.1->coqui_stt_training) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba<=0.53.1->coqui_stt_training) (57.4.0)\n",
            "Requirement already satisfied: scipy>=0.13 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->coqui_stt_training) (1.4.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4->coqui_stt_training) (4.6.3)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from miniaudio->coqui_stt_training) (1.15.0)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna->coqui_stt_training) (1.4.35)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna->coqui_stt_training) (3.13)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna->coqui_stt_training) (21.3)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.7.7-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 KB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->coqui_stt_training) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->coqui_stt_training) (2018.9)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2->coqui_stt_training) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->coqui_stt_training) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->coqui_stt_training) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->coqui_stt_training) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->coqui_stt_training) (3.0.4)\n",
            "Collecting braceexpand\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12.0->miniaudio->coqui_stt_training) (2.21)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.4->coqui_stt_training) (3.1.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna->coqui_stt_training) (3.0.8)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna->coqui_stt_training) (4.11.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna->coqui_stt_training) (1.1.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->coqui_stt_training) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->coqui_stt_training) (3.3.6)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 KB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna->coqui_stt_training) (5.6.0)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.8.1-py2.py3-none-any.whl (113 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.0/113.0 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->coqui_stt_training) (3.2.0)\n",
            "Collecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.0-py3-none-any.whl (29 kB)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.1-py3-none-any.whl (146 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.0/147.0 KB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->coqui_stt_training) (21.4.0)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->coqui_stt_training) (4.1.1)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->coqui_stt_training) (0.2.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna->coqui_stt_training) (3.8.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.4->coqui_stt_training) (1.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna->coqui_stt_training) (2.0.1)\n",
            "Building wheels for collected packages: opuslib, gast, pyogg, pyperclip\n",
            "  Building wheel for opuslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for opuslib: filename=opuslib-2.0.0-py3-none-any.whl size=11009 sha256=35025898e6de2f6cf9cac7856aad798f818c370f158ca35f12dd3426228c5ff2\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/ba/d4/0e81231a9797fbb262ae3a54fd761fab850db7f32d94a3283a\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=f58b41865aea413289b60c344a77d03ae2913ff715d21c5f76be62769362ebf6\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "  Building wheel for pyogg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyogg: filename=PyOgg-0.6.14a1-py2.py3-none-any.whl size=35330 sha256=7e663f7a9bfaea7963504a54b7cd9125b214431240013c2a06d10e926a50d63a\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/aa/6f/5fb54a0a14846e4202945a65937c7f3eb245af5031a141147a\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=39c2d6c4da65e65ba4623a6a473fefafc9a2cd6a699a0693b395779495228a82\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built opuslib gast pyogg pyperclip\n",
            "Installing collected packages: tensorflow-estimator, pyperclip, pyogg, opuslib, braceexpand, pbr, numpy, gast, coqpit, colorlog, autopage, attrdict, webdataset, stevedore, sox, miniaudio, Mako, coqui-stt-ctcdecoder, cmd2, cmaes, tensorboard, keras-applications, cliff, alembic, tensorflow, optuna, coqui_stt_training\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "      Successfully uninstalled tensorflow-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.4 which is incompatible.\n",
            "jaxlib 0.3.2+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n",
            "jax 0.3.4 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.2.0 alembic-1.7.7 attrdict-2.0.1 autopage-0.5.0 braceexpand-0.1.7 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.1 colorlog-6.6.0 coqpit-0.0.15 coqui-stt-ctcdecoder-1.3.0 coqui_stt_training-1.3.0 gast-0.2.2 keras-applications-1.0.8 miniaudio-1.46 numpy-1.18.5 optuna-2.10.0 opuslib-2.0.0 pbr-5.8.1 pyogg-0.6.14a1 pyperclip-1.8.2 sox-1.4.1 stevedore-3.5.0 tensorboard-1.15.0 tensorflow-1.15.4 tensorflow-estimator-1.15.1 webdataset-0.2.5\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "## Install Coqui STT\n",
        "! pip install -U pip\n",
        "! pip install coqui_stt_training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c07a273",
      "metadata": {
        "id": "8c07a273"
      },
      "source": [
        "## ✅ Download pre-trained English model\n",
        "\n",
        "We're going to download a very small (but very accurate) pre-trained STT model for English. This model was trained to only transcribe the English words \"yes\" and \"no\", but with transfer learning we can train a new model which could transcribe any words in any language. In this notebook, we will turn this \"constrained vocabulary\" English model into an \"open vocabulary\" Russian model.\n",
        "\n",
        "Coqui STT models as typically stored as checkpoints (for training) and protobufs (for deployment). For transfer learning, we want the **model checkpoints**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "608d203f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "608d203f",
        "outputId": "a403cf6e-82be-45ac-9c1a-f5f110654364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No path \"english/\" - creating ...\n",
            "No archive \"english/model.tar.gz\" - downloading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1160502/1160502 [00:00<00:00, 26104504.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "No extracted pre-trained model found. Extracting now...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "### Download pre-trained model\n",
        "import os\n",
        "import csv\n",
        "import re\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import wave\n",
        "import contextlib\n",
        "\n",
        "from coqui_stt_training.util.downloader import maybe_download\n",
        "\n",
        "def download_pretrained_model():\n",
        "    model_dir=\"english/\"\n",
        "    if not os.path.exists(\"english/coqui-yesno-checkpoints\"):\n",
        "        maybe_download(\"model.tar.gz\", model_dir, \"https://github.com/coqui-ai/STT-models/releases/download/english%2Fcoqui%2Fyesno-v0.0.1/coqui-yesno-checkpoints.tar.gz\")\n",
        "        print('\\nNo extracted pre-trained model found. Extracting now...')\n",
        "        tar = tarfile.open(\"english/model.tar.gz\")\n",
        "        tar.extractall(\"english/\")\n",
        "        tar.close()\n",
        "    else:\n",
        "        print('Found \"english/coqui-yesno-checkpoints\" - not extracting.')\n",
        "\n",
        "# Download + extract pre-trained English model\n",
        "download_pretrained_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed9dd7ab",
      "metadata": {
        "id": "ed9dd7ab"
      },
      "source": [
        "## ✅ Download data for Russian\n",
        "\n",
        "**First things first**: we need some data.\n",
        "\n",
        "We're training a Speech-to-Text model, so we need some _speech_ and we need some _text_. Specificially, we want _transcribed speech_. Let's download a Russian audio file and its transcript, pre-formatted for 🐸 STT. \n",
        "\n",
        "**Second things second**: we want a Russian alphabet. The output layer of a typical* 🐸 STT model represents letters in the alphabet. Let's download a Russian alphabet from Coqui and use that.\n",
        "\n",
        "*_If you are working with languages with large character sets (e.g. Chinese), you can set `bytes_output_mode=True` instead of supplying an `alphabet.txt` file. In this case, the output layer of the STT model will correspond to individual UTF-8 bytes instead of individual characters._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b5105ea7",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5105ea7",
        "outputId": "dc3243b7-c283-4a95-f903-811bdff14f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No path \"marathi/\" - creating ...\n",
            "No archive \"marathi/data.tgz\" - downloading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5796505/5796505 [00:02<00:00, 2794087.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/\n",
            "data/wavs/\n",
            "data/marathi.tsv\n",
            "data/wavs/test/\n",
            "data/wavs/train/\n",
            "data/wavs/train/._mrt_02624_00000391676.wav\n",
            "data/wavs/train/mrt_02624_00000391676.wav\n",
            "data/wavs/train/._mrt_02436_00013484215.wav\n",
            "data/wavs/train/mrt_02436_00013484215.wav\n",
            "data/wavs/train/._mrt_03349_00062847458.wav\n",
            "data/wavs/train/mrt_03349_00062847458.wav\n",
            "data/wavs/train/._mrt_02484_00002806507.wav\n",
            "data/wavs/train/mrt_02484_00002806507.wav\n",
            "data/wavs/train/._mrt_02484_00007602377.wav\n",
            "data/wavs/train/mrt_02484_00007602377.wav\n",
            "data/wavs/train/._mrt_02624_00007390408.wav\n",
            "data/wavs/train/mrt_02624_00007390408.wav\n",
            "data/wavs/train/._mrt_02436_00013089849.wav\n",
            "data/wavs/train/mrt_02436_00013089849.wav\n",
            "data/wavs/train/._mrt_01523_00028548203.wav\n",
            "data/wavs/train/mrt_01523_00028548203.wav\n",
            "data/wavs/train/._mrt_03349_00062047674.wav\n",
            "data/wavs/train/mrt_03349_00062047674.wav\n",
            "data/wavs/train/._mrt_01523_00029882518.wav\n",
            "data/wavs/train/mrt_01523_00029882518.wav\n",
            "data/wavs/test/._mrt_03397_02119986802.wav\n",
            "data/wavs/test/mrt_03397_02119986802.wav\n",
            "data/wavs/test/._mrt_03398_02142956376.wav\n",
            "data/wavs/test/mrt_03398_02142956376.wav\n",
            "data/wavs/test/._mrt_04310_02117569342.wav\n",
            "data/wavs/test/mrt_04310_02117569342.wav\n",
            "data/wavs/test/._mrt_09697_02139010528.wav\n",
            "data/wavs/test/mrt_09697_02139010528.wav\n",
            "data/wavs/test/._mrt_03398_02145517609.wav\n",
            "data/wavs/test/mrt_03398_02145517609.wav\n",
            "--2022-04-17 13:51:19--  https://github.com/divyankiitb/Asr_files/raw/main/files.zip\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/divyankiitb/Asr_files/main/files.zip [following]\n",
            "--2022-04-17 13:51:20--  https://raw.githubusercontent.com/divyankiitb/Asr_files/main/files.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14446 (14K) [application/zip]\n",
            "Saving to: ‘files.zip’\n",
            "\n",
            "files.zip           100%[===================>]  14.11K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-04-17 13:51:20 (80.4 MB/s) - ‘files.zip’ saved [14446/14446]\n",
            "\n",
            "Archive:  files.zip\n",
            "   creating: files/\n",
            "  inflating: files/mal_alphabet.txt  \n",
            "  inflating: files/alphabet.txt      \n",
            "  inflating: files/kan_alphabet.txt  \n",
            "  inflating: files/generate_lm.py    \n",
            "  inflating: files/kenlm.scorer      \n"
          ]
        }
      ],
      "source": [
        "### Download sample data\n",
        "from coqui_stt_training.util.downloader import maybe_download\n",
        "\n",
        "\n",
        "def download_sample_data():\n",
        "    data_dir=\"marathi/\"\n",
        "    maybe_download(\"data.tgz\",data_dir, \"https://www.cse.iitb.ac.in/~pjyothi/cs753/data.tgz\")\n",
        "    !tar -xzvf \"marathi/data.tgz\" -C \"marathi/\"\n",
        "  \n",
        "    with open(\"marathi/data/marathi.tsv\", 'r') as myfile:  \n",
        "      with open(\"marathi/data/wavs/train/marathi.csv\", 'w') as csv_file:\n",
        "        \n",
        "        count=0\n",
        "        for line in myfile:\n",
        "          \n",
        "          \n",
        "          count=count+1\n",
        "          if(count==11):\n",
        "            break\n",
        "          # Replace every tab with comma\n",
        "          b=os.path.getsize('marathi/data/wavs/train/'+line[0:21]+'.wav')\n",
        "          \n",
        "          fileContent = re.sub(\"\\t\", \".wav,\"+str(b)+\",\", line)\n",
        "        \n",
        "          # Writing into csv file\n",
        "          csv_file.write(fileContent)\n",
        "\n",
        "    with open('marathi/data/wavs/train/marathi.csv',newline='') as f:\n",
        "      r = csv.reader(f)\n",
        "      data = [line for line in r]\n",
        "    with open('marathi/data/wavs/train/marathi.csv','w',newline='') as f:\n",
        "      w = csv.writer(f)\n",
        "      w.writerow(['wav_filename', 'wav_filesize', 'transcript'])\n",
        "      w.writerows(data)     \n",
        "\n",
        "\n",
        "    with open(\"marathi/data/marathi.tsv\", 'r') as myfile:  \n",
        "      with open(\"marathi/data/wavs/test/marathi_test.csv\", 'w') as csv_file:\n",
        "        \n",
        "        count=0\n",
        "        for line in myfile:\n",
        "          \n",
        "          \n",
        "          count=count+1\n",
        "          if(count<11):\n",
        "            continue\n",
        "          if(count==16):\n",
        "            break  \n",
        "          # Replace every tab with comma\n",
        "          b=os.path.getsize('marathi/data/wavs/test/'+line[0:21]+'.wav')\n",
        "          \n",
        "          fileContent = re.sub(\"\\t\", \".wav,\"+str(b)+\",\", line)\n",
        "        \n",
        "          # Writing into csv file\n",
        "          csv_file.write(fileContent)\n",
        "\n",
        "    with open('marathi/data/wavs/test/marathi_test.csv',newline='') as f:\n",
        "      r = csv.reader(f)\n",
        "      data = [line for line in r]\n",
        "    with open('marathi/data/wavs/test/marathi_test.csv','w',newline='') as f:\n",
        "      w = csv.writer(f)\n",
        "      w.writerow(['wav_filename', 'wav_filesize', 'transcript'])\n",
        "      w.writerows(data)\n",
        "\n",
        "    !wget https://github.com/divyankiitb/Asr_files/raw/main/files.zip\n",
        "\n",
        "    !unzip files.zip\n",
        "# Download sample Russian data\n",
        "download_sample_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b46b7227",
      "metadata": {
        "id": "b46b7227"
      },
      "source": [
        "## ✅ Configure the training run\n",
        "\n",
        "Coqui STT comes with a long list of hyperparameters you can tweak. We've set default values, but you can use `initialize_globals_from_args()` to set your own. \n",
        "\n",
        "You must **always** configure the paths to your data, and you must **always** configure your alphabet. For transfer learning, it's good practice to define different `load_checkpoint_dir` and `save_checkpoint_dir` paths so that you keep your new model (Russian STT) separate from the old one (English STT). The parameter `drop_source_layers` allows you to remove layers from the original (aka \"source\") model, and re-initialize them from scratch. If you are fine-tuning to a new alphabet you will have to use _at least_ `drop_source_layers=1` to remove the output layer and add a new output layer which matches your new alphabet.\n",
        "\n",
        "We are fine-tuning a pre-existing model, so `n_hidden` should be the same as the original English model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cff3c5a0",
      "metadata": {
        "id": "cff3c5a0"
      },
      "outputs": [],
      "source": [
        "from coqui_stt_training.util.config import initialize_globals_from_args\n",
        "\n",
        "initialize_globals_from_args(\n",
        "    n_hidden=64,\n",
        "    load_checkpoint_dir=\"english/coqui-yesno-checkpoints\",\n",
        "    save_checkpoint_dir=\"marathi/data/wavs/train/checkpoints\",\n",
        "    drop_source_layers=1,\n",
        "    alphabet_config_path=\"files/alphabet.txt\",\n",
        "    train_files=[\"marathi/data/wavs/train/marathi.csv\"],\n",
        "    dev_files=[\"marathi/data/wavs/train/marathi.csv\"],\n",
        "    epochs=120,\n",
        "    learning_rate=.0019,\n",
        "    dropout_rate=.056,\n",
        "    early_stop=True,\n",
        "    load_cudnn=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "419828c1",
      "metadata": {
        "id": "419828c1"
      },
      "source": [
        "### View all Config settings (*Optional*) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cac6ea3d",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cac6ea3d",
        "outputId": "d80fccdc-2ae7-444c-c287-4ae606057c14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"train_files\": [\n",
            "        \"marathi/data/wavs/train/marathi.csv\"\n",
            "    ],\n",
            "    \"dev_files\": [\n",
            "        \"marathi/data/wavs/train/marathi.csv\"\n",
            "    ],\n",
            "    \"test_files\": [],\n",
            "    \"metrics_files\": [],\n",
            "    \"auto_input_dataset\": \"\",\n",
            "    \"vocab_file\": \"\",\n",
            "    \"read_buffer\": 1048576,\n",
            "    \"feature_cache\": \"\",\n",
            "    \"cache_for_epochs\": 0,\n",
            "    \"shuffle_batches\": false,\n",
            "    \"shuffle_start\": 1,\n",
            "    \"shuffle_buffer\": 1000,\n",
            "    \"feature_win_len\": 32,\n",
            "    \"feature_win_step\": 20,\n",
            "    \"audio_sample_rate\": 16000,\n",
            "    \"normalize_sample_rate\": true,\n",
            "    \"augment\": null,\n",
            "    \"epochs\": 120,\n",
            "    \"dropout_rate\": 0.056,\n",
            "    \"dropout_rate2\": 0.056,\n",
            "    \"dropout_rate3\": 0.056,\n",
            "    \"dropout_rate4\": 0.0,\n",
            "    \"dropout_rate5\": 0.0,\n",
            "    \"dropout_rate6\": 0.056,\n",
            "    \"relu_clip\": 20.0,\n",
            "    \"beta1\": 0.9,\n",
            "    \"beta2\": 0.999,\n",
            "    \"epsilon\": 1e-08,\n",
            "    \"learning_rate\": 0.0019,\n",
            "    \"train_batch_size\": 1,\n",
            "    \"dev_batch_size\": 1,\n",
            "    \"test_batch_size\": 1,\n",
            "    \"export_batch_size\": 1,\n",
            "    \"inter_op_parallelism_threads\": 0,\n",
            "    \"intra_op_parallelism_threads\": 0,\n",
            "    \"use_allow_growth\": false,\n",
            "    \"load_cudnn\": true,\n",
            "    \"train_cudnn\": false,\n",
            "    \"automatic_mixed_precision\": false,\n",
            "    \"limit_test\": 0,\n",
            "    \"reverse_test\": false,\n",
            "    \"checkpoint_dir\": \"\",\n",
            "    \"load_checkpoint_dir\": \"english/coqui-yesno-checkpoints\",\n",
            "    \"save_checkpoint_dir\": \"marathi/data/wavs/train/checkpoints\",\n",
            "    \"checkpoint_secs\": 600,\n",
            "    \"max_to_keep\": 5,\n",
            "    \"load_train\": \"auto\",\n",
            "    \"load_evaluate\": \"auto\",\n",
            "    \"drop_source_layers\": 1,\n",
            "    \"export_dir\": \"\",\n",
            "    \"remove_export\": false,\n",
            "    \"export_tflite\": true,\n",
            "    \"export_quantize\": true,\n",
            "    \"export_savedmodel\": false,\n",
            "    \"n_steps\": 16,\n",
            "    \"export_zip\": false,\n",
            "    \"export_file_name\": \"output_graph\",\n",
            "    \"export_beam_width\": 500,\n",
            "    \"export_author_id\": \"author\",\n",
            "    \"export_model_name\": \"model\",\n",
            "    \"export_model_version\": \"0.0.1\",\n",
            "    \"export_contact_info\": \"<public contact information of the author. Can be an email address, or a link to a contact form, issue tracker, or discussion forum. Must provide a way to reach the model authors>\",\n",
            "    \"export_license\": \"<SPDX identifier of the license of the exported model. See https://spdx.org/licenses/. If the license does not have an SPDX identifier, use the license name.>\",\n",
            "    \"export_language\": \"<language the model was trained on - IETF BCP 47 language tag including at least language, script and region subtags. E.g. \\\"en-Latn-UK\\\" or \\\"de-Latn-DE\\\" or \\\"cmn-Hans-CN\\\". Include as much info as you can without loss of precision. For example, if a model is trained on Scottish English, include the variant subtag: \\\"en-Latn-GB-Scotland\\\".>\",\n",
            "    \"export_min_stt_version\": \"<minimum Coqui STT version (inclusive) the exported model is compatible with>\",\n",
            "    \"export_max_stt_version\": \"<maximum Coqui STT version (inclusive) the exported model is compatible with>\",\n",
            "    \"export_description\": \"<Freeform description of the model being exported. Markdown accepted. You can also leave this flag unchanged and edit the generated .md file directly. Useful things to describe are demographic and acoustic characteristics of the data used to train the model, any architectural changes, names of public datasets that were used when applicable, hyperparameters used for training, evaluation results on standard benchmark datasets, etc.>\",\n",
            "    \"log_level\": 1,\n",
            "    \"show_progressbar\": true,\n",
            "    \"log_placement\": false,\n",
            "    \"report_count\": 5,\n",
            "    \"summary_dir\": \"marathi/data/wavs/train/checkpoints/summaries\",\n",
            "    \"test_output_file\": \"\",\n",
            "    \"n_hidden\": 64,\n",
            "    \"layer_norm\": false,\n",
            "    \"random_seed\": 4568,\n",
            "    \"early_stop\": true,\n",
            "    \"es_epochs\": 25,\n",
            "    \"es_min_delta\": 0.05,\n",
            "    \"reduce_lr_on_plateau\": false,\n",
            "    \"plateau_epochs\": 10,\n",
            "    \"plateau_reduction\": 0.1,\n",
            "    \"force_initialize_learning_rate\": false,\n",
            "    \"bytes_output_mode\": false,\n",
            "    \"alphabet_config_path\": \"files/alphabet.txt\",\n",
            "    \"scorer_path\": \"\",\n",
            "    \"beam_width\": 1024,\n",
            "    \"lm_alpha\": 0.931289039105002,\n",
            "    \"lm_beta\": 1.1834137581510284,\n",
            "    \"cutoff_prob\": 1.0,\n",
            "    \"cutoff_top_n\": 300,\n",
            "    \"one_shot_infer\": null,\n",
            "    \"lm_alpha_max\": 5,\n",
            "    \"lm_beta_max\": 5,\n",
            "    \"n_trials\": 2400\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from coqui_stt_training.util.config import Config\n",
        "\n",
        "print(Config.to_json())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8e700d1",
      "metadata": {
        "id": "c8e700d1"
      },
      "source": [
        "## ✅ Train a new Russian model\n",
        "\n",
        "Let's kick off a training run 🚀🚀🚀 (using the configure you set above).\n",
        "\n",
        "This notebook should work on either a GPU or a CPU. However, in case you're running this on _multiple_ GPUs we want to only use one, because the sample dataset (one audio file) is too small to split across multiple GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8aab2195",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aab2195",
        "outputId": "f495e44c-51bc-467e-b380-aa03f8f53745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I Performing dummy training to check for memory problems.\n",
            "I If the following process crashes, you likely have batch sizes that are too big for your available system memory (or GPU memory).\n",
            "I Loading best validating checkpoint from english/coqui-yesno-checkpoints/best_dev-1909\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam_1\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam_1\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam\n",
            "I Loading variable from checkpoint: beta1_power\n",
            "I Loading variable from checkpoint: beta2_power\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam_1\n",
            "I Loading variable from checkpoint: learning_rate\n",
            "I Initializing variable: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam\n",
            "I Initializing variable: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam_1\n",
            "I Initializing variable: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam\n",
            "I Initializing variable: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam_1\n",
            "I Initializing variable: layer_6/bias\n",
            "I Initializing variable: layer_6/bias/Adam\n",
            "I Initializing variable: layer_6/bias/Adam_1\n",
            "I Initializing variable: layer_6/weights\n",
            "I Initializing variable: layer_6/weights/Adam\n",
            "I Initializing variable: layer_6/weights/Adam_1\n",
            "I STARTING Optimization\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:01 | Steps: 3 | Loss: 1349.460734    \n",
            "Epoch 0 | Validation | Elapsed Time: 0:00:01 | Steps: 3 | Loss: 853.184631 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "I FINISHED optimization in 0:00:03.256623\n",
            "I Dummy run finished without problems, now starting real training process.\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam_1\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam_1\n",
            "I STARTING Optimization\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 784.880518    \n",
            "Epoch 0 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 465.707053 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 465.707053 to: marathi/data/wavs/train/checkpoints/best_dev-1919\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 413.913669    \n",
            "Epoch 1 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 281.362510 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 281.362510 to: marathi/data/wavs/train/checkpoints/best_dev-1929\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 251.977487    \n",
            "Epoch 2 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 244.296093 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 244.296093 to: marathi/data/wavs/train/checkpoints/best_dev-1939\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 253.331522    \n",
            "Epoch 3 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 233.901768 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 233.901768 to: marathi/data/wavs/train/checkpoints/best_dev-1949\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 233.847469    \n",
            "Epoch 4 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 230.063738 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 230.063738 to: marathi/data/wavs/train/checkpoints/best_dev-1959\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 230.568275    \n",
            "Epoch 5 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 224.717731 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 224.717731 to: marathi/data/wavs/train/checkpoints/best_dev-1969\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 227.182264    \n",
            "Epoch 6 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 220.518040 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 220.518040 to: marathi/data/wavs/train/checkpoints/best_dev-1979\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 231.027219    \n",
            "Epoch 7 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 222.410529 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 8 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 218.888597    \n",
            "Epoch 8 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 225.186820 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 9 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 226.915556    \n",
            "Epoch 9 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 216.123452 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 216.123452 to: marathi/data/wavs/train/checkpoints/best_dev-2009\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 10 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 218.477560   \n",
            "Epoch 10 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 214.669508 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 214.669508 to: marathi/data/wavs/train/checkpoints/best_dev-2019\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 11 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 219.010941   \n",
            "Epoch 11 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 214.017380 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 214.017380 to: marathi/data/wavs/train/checkpoints/best_dev-2029\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 12 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 216.214950   \n",
            "Epoch 12 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 215.808435 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 13 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 220.507263   \n",
            "Epoch 13 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 209.645568 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 209.645568 to: marathi/data/wavs/train/checkpoints/best_dev-2049\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 14 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 209.740454   \n",
            "Epoch 14 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 209.446118 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 209.446118 to: marathi/data/wavs/train/checkpoints/best_dev-2059\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 15 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 210.081541   \n",
            "Epoch 15 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 209.015849 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 209.015849 to: marathi/data/wavs/train/checkpoints/best_dev-2069\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 16 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 213.856701   \n",
            "Epoch 16 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 212.972495 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 17 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 209.777555   \n",
            "Epoch 17 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 204.687292 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 204.687292 to: marathi/data/wavs/train/checkpoints/best_dev-2089\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 18 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 205.196511   \n",
            "Epoch 18 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 204.977917 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 19 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 204.690076   \n",
            "Epoch 19 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 204.627275 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 204.627275 to: marathi/data/wavs/train/checkpoints/best_dev-2109\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 20 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 202.311320   \n",
            "Epoch 20 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 199.544471 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 199.544471 to: marathi/data/wavs/train/checkpoints/best_dev-2119\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 21 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 199.647362   \n",
            "Epoch 21 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 200.451055 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 22 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 201.248460   \n",
            "Epoch 22 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 194.112323 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 194.112323 to: marathi/data/wavs/train/checkpoints/best_dev-2139\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 23 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 197.088238   \n",
            "Epoch 23 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 196.672367 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 24 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 195.050470   \n",
            "Epoch 24 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 196.762351 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 25 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 196.743831   \n",
            "Epoch 25 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 190.564979 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 190.564979 to: marathi/data/wavs/train/checkpoints/best_dev-2169\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 26 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 192.785629   \n",
            "Epoch 26 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 193.873332 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 27 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 193.826761   \n",
            "Epoch 27 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 189.720816 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 189.720816 to: marathi/data/wavs/train/checkpoints/best_dev-2189\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 28 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 191.738339   \n",
            "Epoch 28 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 194.811287 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 29 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 194.385938   \n",
            "Epoch 29 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 185.644427 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 185.644427 to: marathi/data/wavs/train/checkpoints/best_dev-2209\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 30 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 190.885201   \n",
            "Epoch 30 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 191.216293 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 31 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 194.378761   \n",
            "Epoch 31 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 184.356097 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 184.356097 to: marathi/data/wavs/train/checkpoints/best_dev-2229\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 32 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 190.404471   \n",
            "Epoch 32 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 185.526549 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 33 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 189.273648   \n",
            "Epoch 33 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 188.050460 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 34 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 187.401467   \n",
            "Epoch 34 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 187.582510 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 35 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 189.223542   \n",
            "Epoch 35 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 182.869870 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 182.869870 to: marathi/data/wavs/train/checkpoints/best_dev-2269\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 36 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 187.438123   \n",
            "Epoch 36 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 185.654784 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 37 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 186.244404   \n",
            "Epoch 37 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 183.488609 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 38 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 186.145918   \n",
            "Epoch 38 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 185.207584 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 39 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 185.262670   \n",
            "Epoch 39 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 185.832504 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 40 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 185.936813   \n",
            "Epoch 40 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 183.311098 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 41 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 185.672353   \n",
            "Epoch 41 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 186.426096 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 42 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 187.339086   \n",
            "Epoch 42 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 179.558657 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 179.558657 to: marathi/data/wavs/train/checkpoints/best_dev-2339\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 43 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 181.733680   \n",
            "Epoch 43 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 178.329341 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 178.329341 to: marathi/data/wavs/train/checkpoints/best_dev-2349\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 44 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 179.971683   \n",
            "Epoch 44 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 180.845139 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 45 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 181.313710   \n",
            "Epoch 45 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 175.783102 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 175.783102 to: marathi/data/wavs/train/checkpoints/best_dev-2369\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 46 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 180.233192   \n",
            "Epoch 46 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 176.238776 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 47 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 177.431976   \n",
            "Epoch 47 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 174.367974 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 174.367974 to: marathi/data/wavs/train/checkpoints/best_dev-2389\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 48 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 177.163938   \n",
            "Epoch 48 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 173.998461 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 173.998461 to: marathi/data/wavs/train/checkpoints/best_dev-2399\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 49 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 177.030875   \n",
            "Epoch 49 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 173.626390 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 173.626390 to: marathi/data/wavs/train/checkpoints/best_dev-2409\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 50 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 177.020251   \n",
            "Epoch 50 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 171.473445 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 171.473445 to: marathi/data/wavs/train/checkpoints/best_dev-2419\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 51 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 175.359735   \n",
            "Epoch 51 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 173.327828 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 52 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 175.231878   \n",
            "Epoch 52 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 173.652929 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 53 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 174.824586   \n",
            "Epoch 53 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 172.544046 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 54 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 174.871315   \n",
            "Epoch 54 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 177.053933 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 55 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 180.670428   \n",
            "Epoch 55 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 177.621050 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 56 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 176.390340   \n",
            "Epoch 56 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 176.321020 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 57 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 176.675329   \n",
            "Epoch 57 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 174.414597 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 58 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 176.033068   \n",
            "Epoch 58 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 170.003662 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 170.003662 to: marathi/data/wavs/train/checkpoints/best_dev-2499\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 59 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 172.453658   \n",
            "Epoch 59 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 173.346473 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 60 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 170.884840   \n",
            "Epoch 60 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 167.562003 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 167.562003 to: marathi/data/wavs/train/checkpoints/best_dev-2519\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 61 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 171.088712   \n",
            "Epoch 61 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 166.511623 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 166.511623 to: marathi/data/wavs/train/checkpoints/best_dev-2529\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 62 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 170.170550   \n",
            "Epoch 62 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 167.782267 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 63 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 168.783057   \n",
            "Epoch 63 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 165.377525 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 165.377525 to: marathi/data/wavs/train/checkpoints/best_dev-2549\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 64 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 167.427502   \n",
            "Epoch 64 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 164.830076 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 164.830076 to: marathi/data/wavs/train/checkpoints/best_dev-2559\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 65 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 167.339202   \n",
            "Epoch 65 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 164.579523 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 164.579523 to: marathi/data/wavs/train/checkpoints/best_dev-2569\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 66 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 165.916660   \n",
            "Epoch 66 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 162.027251 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 162.027251 to: marathi/data/wavs/train/checkpoints/best_dev-2579\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 67 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 165.437568   \n",
            "Epoch 67 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 163.928114 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 68 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 168.228461   \n",
            "Epoch 68 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 160.871949 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 160.871949 to: marathi/data/wavs/train/checkpoints/best_dev-2599\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 69 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 163.377861   \n",
            "Epoch 69 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 162.454404 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 70 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 165.546453   \n",
            "Epoch 70 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 160.184705 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 160.184705 to: marathi/data/wavs/train/checkpoints/best_dev-2619\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 71 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 169.229526   \n",
            "Epoch 71 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 160.482825 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 72 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 166.626357   \n",
            "Epoch 72 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 164.320997 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 73 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 163.985483   \n",
            "Epoch 73 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 159.870995 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 159.870995 to: marathi/data/wavs/train/checkpoints/best_dev-2649\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 74 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 163.803922   \n",
            "Epoch 74 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 159.106801 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 159.106801 to: marathi/data/wavs/train/checkpoints/best_dev-2659\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 75 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 164.697903   \n",
            "Epoch 75 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 158.886953 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 158.886953 to: marathi/data/wavs/train/checkpoints/best_dev-2669\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 76 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 162.138153   \n",
            "Epoch 76 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 160.505362 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 77 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 162.632097   \n",
            "Epoch 77 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 155.738702 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 155.738702 to: marathi/data/wavs/train/checkpoints/best_dev-2689\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 78 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 164.353442   \n",
            "Epoch 78 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 154.995982 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 154.995982 to: marathi/data/wavs/train/checkpoints/best_dev-2699\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 79 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 160.076960   \n",
            "Epoch 79 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 154.535262 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 154.535262 to: marathi/data/wavs/train/checkpoints/best_dev-2709\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 80 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 159.382178   \n",
            "Epoch 80 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 154.852000 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 81 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 162.185112   \n",
            "Epoch 81 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 156.119115 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 82 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 161.223782   \n",
            "Epoch 82 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 153.211211 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 153.211211 to: marathi/data/wavs/train/checkpoints/best_dev-2739\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 83 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 158.982189   \n",
            "Epoch 83 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 156.856612 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 84 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 161.035169   \n",
            "Epoch 84 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 151.840755 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 151.840755 to: marathi/data/wavs/train/checkpoints/best_dev-2759\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 85 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 158.116160   \n",
            "Epoch 85 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 152.208038 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 86 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 158.272597   \n",
            "Epoch 86 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 150.599902 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 150.599902 to: marathi/data/wavs/train/checkpoints/best_dev-2779\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 87 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 156.424576   \n",
            "Epoch 87 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 155.060061 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 88 |   Training | Elapsed Time: 0:00:03 | Steps: 10 | Loss: 158.696948   \n",
            "Epoch 88 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 150.301325 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 150.301325 to: marathi/data/wavs/train/checkpoints/best_dev-2799\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 89 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 160.898777   \n",
            "Epoch 89 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 161.987043 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 90 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 165.242067   \n",
            "Epoch 90 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 152.846912 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 91 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 162.703197   \n",
            "Epoch 91 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 153.105716 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 92 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 161.786043   \n",
            "Epoch 92 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 156.531926 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 93 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 157.614093   \n",
            "Epoch 93 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 149.626307 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 149.626307 to: marathi/data/wavs/train/checkpoints/best_dev-2849\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 94 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 155.425992   \n",
            "Epoch 94 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 149.810193 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 95 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 154.976550   \n",
            "Epoch 95 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 148.260123 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 148.260123 to: marathi/data/wavs/train/checkpoints/best_dev-2869\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 96 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 152.750475   \n",
            "Epoch 96 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 147.528741 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 147.528741 to: marathi/data/wavs/train/checkpoints/best_dev-2879\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 97 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 153.141035   \n",
            "Epoch 97 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 145.946075 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 145.946075 to: marathi/data/wavs/train/checkpoints/best_dev-2889\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 98 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 153.155174   \n",
            "Epoch 98 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 145.055796 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 145.055796 to: marathi/data/wavs/train/checkpoints/best_dev-2899\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 99 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 152.540679   \n",
            "Epoch 99 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 144.687584 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 144.687584 to: marathi/data/wavs/train/checkpoints/best_dev-2909\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 100 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 150.423161  \n",
            "Epoch 100 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 144.027264 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 144.027264 to: marathi/data/wavs/train/checkpoints/best_dev-2919\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 101 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 150.558492  \n",
            "Epoch 101 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 143.149847 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 143.149847 to: marathi/data/wavs/train/checkpoints/best_dev-2929\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 102 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 150.472807  \n",
            "Epoch 102 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 142.355764 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 142.355764 to: marathi/data/wavs/train/checkpoints/best_dev-2939\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 103 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 147.758142  \n",
            "Epoch 103 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 143.476707 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 104 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 148.969353  \n",
            "Epoch 104 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 141.362193 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 141.362193 to: marathi/data/wavs/train/checkpoints/best_dev-2959\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 105 |   Training | Elapsed Time: 0:00:03 | Steps: 10 | Loss: 149.148064  \n",
            "Epoch 105 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 140.958355 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 140.958355 to: marathi/data/wavs/train/checkpoints/best_dev-2969\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 106 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 149.176244  \n",
            "Epoch 106 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 141.933678 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 107 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 148.570649  \n",
            "Epoch 107 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 141.685860 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 108 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 149.284441  \n",
            "Epoch 108 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 140.438235 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 140.438235 to: marathi/data/wavs/train/checkpoints/best_dev-2999\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 109 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 147.118201  \n",
            "Epoch 109 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 138.619572 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 138.619572 to: marathi/data/wavs/train/checkpoints/best_dev-3009\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 110 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 145.218622  \n",
            "Epoch 110 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 141.405665 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 111 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 148.894959  \n",
            "Epoch 111 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 138.109995 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 138.109995 to: marathi/data/wavs/train/checkpoints/best_dev-3029\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 112 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 146.133120  \n",
            "Epoch 112 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 140.064932 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 113 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 146.257107  \n",
            "Epoch 113 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 136.256815 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 136.256815 to: marathi/data/wavs/train/checkpoints/best_dev-3049\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 114 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 143.673940  \n",
            "Epoch 114 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 135.535753 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 135.535753 to: marathi/data/wavs/train/checkpoints/best_dev-3059\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 115 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 140.956110  \n",
            "Epoch 115 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 136.034866 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 116 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 144.726441  \n",
            "Epoch 116 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 135.285580 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 135.285580 to: marathi/data/wavs/train/checkpoints/best_dev-3079\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 117 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 143.395592  \n",
            "Epoch 117 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 136.923673 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 118 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 141.769250  \n",
            "Epoch 118 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 135.136002 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 135.136002 to: marathi/data/wavs/train/checkpoints/best_dev-3099\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 119 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 143.400903  \n",
            "Epoch 119 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 135.487257 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 120 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 143.131683  \n",
            "Epoch 120 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 136.815140 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 121 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 151.858068  \n",
            "Epoch 121 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 156.443996 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 122 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 160.726749  \n",
            "Epoch 122 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 162.514940 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 123 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 165.310880  \n",
            "Epoch 123 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 153.828116 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 124 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 151.197489  \n",
            "Epoch 124 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 141.448887 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "I FINISHED optimization in 0:09:25.484241\n"
          ]
        }
      ],
      "source": [
        "from coqui_stt_training.train import train\n",
        "\n",
        "# use maximum one GPU\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c87ba61",
      "metadata": {
        "id": "3c87ba61"
      },
      "source": [
        "## ✅ Configure the testing run\n",
        "\n",
        "Let's add the path to our testing data and update `load_checkpoint_dir` to our new model checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2be7beb5",
      "metadata": {
        "id": "2be7beb5"
      },
      "outputs": [],
      "source": [
        "from coqui_stt_training.util.config import Config\n",
        "\n",
        "Config.test_files=[\"marathi/data/wavs/test/marathi_test.csv\"]\n",
        "Config.load_checkpoint_dir=\"marathi/data/wavs/train/checkpoints\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6a5c971",
      "metadata": {
        "id": "c6a5c971"
      },
      "source": [
        "## ✅ Test the new Russian model\n",
        "\n",
        "We made it! 🙌\n",
        "\n",
        "Let's kick off the testing run, which displays performance metrics.\n",
        "\n",
        "We're committing the cardinal sin of ML 😈 (aka - testing on our training data) so you don't want to deploy this model into production. In this notebook we're focusing on the workflow itself, so it's forgivable 😇\n",
        "\n",
        "You can see from the test output that our tiny model has overfit to the data, and basically memorized this one sentence.\n",
        "\n",
        "When you start training your own models, make sure your testing data doesn't include your training data 😅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6222dc69",
      "metadata": {
        "id": "6222dc69"
      },
      "outputs": [],
      "source": [
        "from coqui_stt_training.evaluate import test\n",
        "\n",
        "test()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "final_of easy_transfer_learning.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}