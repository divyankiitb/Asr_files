{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyankiitb/Asr_files/blob/main/final_of_easy_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45ea3ef5",
      "metadata": {
        "id": "45ea3ef5"
      },
      "source": [
        "# Easy transfer learning with üê∏ STT ‚ö°\n",
        "\n",
        "You want to train a Coqui (üê∏) STT model, but you don't have a lot of data. What do you do?\n",
        "\n",
        "The answer üí°: Grab a pre-trained model and fine-tune it to your data. This is called `\"Transfer Learning\"` ‚ö°\n",
        "\n",
        "üê∏ STT comes with transfer learning support out-of-the box.\n",
        "\n",
        "You can even take a pre-trained model and fine-tune it to _any new language_, even if the alphabets are completely different. Likewise, you can fine-tune a model to your own data and improve performance if the language is the same.\n",
        "\n",
        "In this notebook, we will:\n",
        "\n",
        "1. Download a pre-trained English STT model.\n",
        "2. Download data for the Russian language.\n",
        "3. Fine-tune the English model to Russian language.\n",
        "4. Test the new Russian model and display its performance.\n",
        "\n",
        "So, let's jump right in!\n",
        "\n",
        "*PS - If you just want a working, off-the-shelf model, check out the [üê∏ Model Zoo](https://www.coqui.ai/models)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa2aec77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa2aec77",
        "outputId": "656ebaf4-0eb4-4c8d-9f5d-4d19fd1f7b8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-22.0.4-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.1 MB 7.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-22.0.4\n",
            "Collecting coqui_stt_training\n",
            "  Downloading coqui_stt_training-1.3.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m87.2/87.2 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coqpit\n",
            "  Downloading coqpit-0.0.15-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (0.10.3.post1)\n",
            "Collecting sox\n",
            "  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n",
            "Collecting optuna\n",
            "  Downloading optuna-2.10.0-py3-none-any.whl (308 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m308.2/308.2 KB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opuslib==2.0.0\n",
            "  Downloading opuslib-2.0.0.tar.gz (7.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting coqui-stt-ctcdecoder==1.3.0\n",
            "  Downloading coqui_stt_ctcdecoder-1.3.0-cp37-cp37m-manylinux_2_24_x86_64.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyogg>=0.6.14a1\n",
            "  Downloading PyOgg-0.6.14a1.tar.gz (35 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (3.38.0)\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (2.13.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (1.21.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (4.64.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (0.2.2)\n",
            "Collecting miniaudio\n",
            "  Downloading miniaudio-1.46-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (551 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m551.8/551.8 KB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (0.0.1)\n",
            "Collecting webdataset\n",
            "  Downloading webdataset-0.2.5-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.9/46.9 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (2.23.0)\n",
            "Requirement already satisfied: numba<=0.53.1 in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (0.51.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (1.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from coqui_stt_training) (1.3.5)\n",
            "Collecting attrdict\n",
            "  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Collecting tensorflow==1.15.4\n",
            "  Downloading tensorflow-1.15.4-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m110.5/110.5 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (1.44.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (1.1.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (3.3.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m503.4/503.4 KB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (0.37.1)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (1.14.0)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.7/50.7 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui_stt_training) (1.0.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<=0.53.1->coqui_stt_training) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba<=0.53.1->coqui_stt_training) (57.4.0)\n",
            "Requirement already satisfied: scipy>=0.13 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->coqui_stt_training) (1.4.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4->coqui_stt_training) (4.6.3)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from miniaudio->coqui_stt_training) (1.15.0)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.7.7-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m210.7/210.7 KB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog\n",
            "  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna->coqui_stt_training) (21.3)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna->coqui_stt_training) (1.4.35)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m81.0/81.0 KB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna->coqui_stt_training) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->coqui_stt_training) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->coqui_stt_training) (2018.9)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2->coqui_stt_training) (3.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->coqui_stt_training) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->coqui_stt_training) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->coqui_stt_training) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->coqui_stt_training) (2.10)\n",
            "Collecting braceexpand\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12.0->miniaudio->coqui_stt_training) (2.21)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.4->coqui_stt_training) (3.1.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna->coqui_stt_training) (3.0.8)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna->coqui_stt_training) (1.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna->coqui_stt_training) (4.11.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->coqui_stt_training) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->coqui_stt_training) (1.0.1)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.4/78.4 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna->coqui_stt_training) (5.6.0)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->coqui_stt_training) (3.2.0)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.1-py3-none-any.whl (146 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m147.0/147.0 KB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.8.1-py2.py3-none-any.whl (113 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m113.0/113.0 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.0-py3-none-any.whl (29 kB)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.7/49.7 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->coqui_stt_training) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->coqui_stt_training) (4.1.1)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->coqui_stt_training) (21.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna->coqui_stt_training) (3.8.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.4->coqui_stt_training) (1.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna->coqui_stt_training) (2.0.1)\n",
            "Building wheels for collected packages: opuslib, gast, pyogg, pyperclip\n",
            "  Building wheel for opuslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for opuslib: filename=opuslib-2.0.0-py3-none-any.whl size=11009 sha256=f7287b3feced9aff8a845712399a5f22ba8e48c89ac05726fc1a38a8bc2ff529\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/ba/d4/0e81231a9797fbb262ae3a54fd761fab850db7f32d94a3283a\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=928e1eedee7f5f42f37388f4f0b1f88a006525328103179f511982aaf8cb4ea4\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "  Building wheel for pyogg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyogg: filename=PyOgg-0.6.14a1-py2.py3-none-any.whl size=35330 sha256=fe14b43d3f69106d3d85006fa478e70e285bca9e9f2b364dfd3f2eafa1ccf5cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/aa/6f/5fb54a0a14846e4202945a65937c7f3eb245af5031a141147a\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=87a6388bf80a95af41f097cf40aad88c159fc13ec7e3c89cf5c28b86f49940f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built opuslib gast pyogg pyperclip\n",
            "Installing collected packages: tensorflow-estimator, pyperclip, pyogg, opuslib, braceexpand, pbr, numpy, gast, coqpit, colorlog, autopage, attrdict, webdataset, stevedore, sox, miniaudio, Mako, coqui-stt-ctcdecoder, cmd2, cmaes, tensorboard, keras-applications, cliff, alembic, tensorflow, optuna, coqui_stt_training\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "      Successfully uninstalled tensorflow-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.4 which is incompatible.\n",
            "jaxlib 0.3.2+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n",
            "jax 0.3.4 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.2.0 alembic-1.7.7 attrdict-2.0.1 autopage-0.5.0 braceexpand-0.1.7 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.1 colorlog-6.6.0 coqpit-0.0.15 coqui-stt-ctcdecoder-1.3.0 coqui_stt_training-1.3.0 gast-0.2.2 keras-applications-1.0.8 miniaudio-1.46 numpy-1.18.5 optuna-2.10.0 opuslib-2.0.0 pbr-5.8.1 pyogg-0.6.14a1 pyperclip-1.8.2 sox-1.4.1 stevedore-3.5.0 tensorboard-1.15.0 tensorflow-1.15.4 tensorflow-estimator-1.15.1 webdataset-0.2.5\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "## Install Coqui STT\n",
        "! pip install -U pip\n",
        "! pip install coqui_stt_training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c07a273",
      "metadata": {
        "id": "8c07a273"
      },
      "source": [
        "## ‚úÖ Download pre-trained English model\n",
        "\n",
        "We're going to download a very small (but very accurate) pre-trained STT model for English. This model was trained to only transcribe the English words \"yes\" and \"no\", but with transfer learning we can train a new model which could transcribe any words in any language. In this notebook, we will turn this \"constrained vocabulary\" English model into an \"open vocabulary\" Russian model.\n",
        "\n",
        "Coqui STT models as typically stored as checkpoints (for training) and protobufs (for deployment). For transfer learning, we want the **model checkpoints**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "608d203f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "608d203f",
        "outputId": "cb4ec3cd-51ec-42cc-f17b-ed6aafedc0da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No path \"english/\" - creating ...\n",
            "No archive \"english/model.tar.gz\" - downloading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1160502/1160502 [00:00<00:00, 29793408.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "No extracted pre-trained model found. Extracting now...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "### Download pre-trained model\n",
        "import os\n",
        "import csv\n",
        "import re\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import wave\n",
        "import contextlib\n",
        "\n",
        "from coqui_stt_training.util.downloader import maybe_download\n",
        "\n",
        "def download_pretrained_model():\n",
        "    model_dir=\"english/\"\n",
        "    if not os.path.exists(\"english/coqui-yesno-checkpoints\"):\n",
        "        maybe_download(\"model.tar.gz\", model_dir, \"https://github.com/coqui-ai/STT-models/releases/download/english%2Fcoqui%2Fyesno-v0.0.1/coqui-yesno-checkpoints.tar.gz\")\n",
        "        print('\\nNo extracted pre-trained model found. Extracting now...')\n",
        "        tar = tarfile.open(\"english/model.tar.gz\")\n",
        "        tar.extractall(\"english/\")\n",
        "        tar.close()\n",
        "    else:\n",
        "        print('Found \"english/coqui-yesno-checkpoints\" - not extracting.')\n",
        "\n",
        "# Download + extract pre-trained English model\n",
        "download_pretrained_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed9dd7ab",
      "metadata": {
        "id": "ed9dd7ab"
      },
      "source": [
        "## ‚úÖ Download data for Russian\n",
        "\n",
        "**First things first**: we need some data.\n",
        "\n",
        "We're training a Speech-to-Text model, so we need some _speech_ and we need some _text_. Specificially, we want _transcribed speech_. Let's download a Russian audio file and its transcript, pre-formatted for üê∏ STT. \n",
        "\n",
        "**Second things second**: we want a Russian alphabet. The output layer of a typical* üê∏ STT model represents letters in the alphabet. Let's download a Russian alphabet from Coqui and use that.\n",
        "\n",
        "*_If you are working with languages with large character sets (e.g. Chinese), you can set `bytes_output_mode=True` instead of supplying an `alphabet.txt` file. In this case, the output layer of the STT model will correspond to individual UTF-8 bytes instead of individual characters._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5105ea7",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5105ea7",
        "outputId": "49dd2c63-761e-408e-b3fa-45f3e893364c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No path \"marathi/\" - creating ...\n",
            "No archive \"marathi/data.tgz\" - downloading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5796505/5796505 [00:01<00:00, 3518189.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/\n",
            "data/wavs/\n",
            "data/marathi.tsv\n",
            "data/wavs/test/\n",
            "data/wavs/train/\n",
            "data/wavs/train/._mrt_02624_00000391676.wav\n",
            "data/wavs/train/mrt_02624_00000391676.wav\n",
            "data/wavs/train/._mrt_02436_00013484215.wav\n",
            "data/wavs/train/mrt_02436_00013484215.wav\n",
            "data/wavs/train/._mrt_03349_00062847458.wav\n",
            "data/wavs/train/mrt_03349_00062847458.wav\n",
            "data/wavs/train/._mrt_02484_00002806507.wav\n",
            "data/wavs/train/mrt_02484_00002806507.wav\n",
            "data/wavs/train/._mrt_02484_00007602377.wav\n",
            "data/wavs/train/mrt_02484_00007602377.wav\n",
            "data/wavs/train/._mrt_02624_00007390408.wav\n",
            "data/wavs/train/mrt_02624_00007390408.wav\n",
            "data/wavs/train/._mrt_02436_00013089849.wav\n",
            "data/wavs/train/mrt_02436_00013089849.wav\n",
            "data/wavs/train/._mrt_01523_00028548203.wav\n",
            "data/wavs/train/mrt_01523_00028548203.wav\n",
            "data/wavs/train/._mrt_03349_00062047674.wav\n",
            "data/wavs/train/mrt_03349_00062047674.wav\n",
            "data/wavs/train/._mrt_01523_00029882518.wav\n",
            "data/wavs/train/mrt_01523_00029882518.wav\n",
            "data/wavs/test/._mrt_03397_02119986802.wav\n",
            "data/wavs/test/mrt_03397_02119986802.wav\n",
            "data/wavs/test/._mrt_03398_02142956376.wav\n",
            "data/wavs/test/mrt_03398_02142956376.wav\n",
            "data/wavs/test/._mrt_04310_02117569342.wav\n",
            "data/wavs/test/mrt_04310_02117569342.wav\n",
            "data/wavs/test/._mrt_09697_02139010528.wav\n",
            "data/wavs/test/mrt_09697_02139010528.wav\n",
            "data/wavs/test/._mrt_03398_02145517609.wav\n",
            "data/wavs/test/mrt_03398_02145517609.wav\n",
            "--2022-04-17 10:10:39--  https://github.com/divyankiitb/Asr_files/raw/main/files.zip\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/divyankiitb/Asr_files/main/files.zip [following]\n",
            "--2022-04-17 10:10:39--  https://raw.githubusercontent.com/divyankiitb/Asr_files/main/files.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3478 (3.4K) [application/zip]\n",
            "Saving to: ‚Äòfiles.zip‚Äô\n",
            "\n",
            "files.zip           100%[===================>]   3.40K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-04-17 10:10:40 (59.9 MB/s) - ‚Äòfiles.zip‚Äô saved [3478/3478]\n",
            "\n",
            "Archive:  files.zip\n",
            "   creating: files/\n",
            "  inflating: files/mal_alphabet.txt  \n",
            "  inflating: files/alphabet.txt      \n",
            "  inflating: files/kan_alphabet.txt  \n",
            "  inflating: files/generate_lm.py    \n"
          ]
        }
      ],
      "source": [
        "### Download sample data\n",
        "from coqui_stt_training.util.downloader import maybe_download\n",
        "\n",
        "\n",
        "def download_sample_data():\n",
        "    data_dir=\"marathi/\"\n",
        "    maybe_download(\"data.tgz\",data_dir, \"https://www.cse.iitb.ac.in/~pjyothi/cs753/data.tgz\")\n",
        "    !tar -xzvf \"marathi/data.tgz\" -C \"marathi/\"\n",
        "  \n",
        "    with open(\"marathi/data/marathi.tsv\", 'r') as myfile:  \n",
        "      with open(\"marathi/data/wavs/train/marathi.csv\", 'w') as csv_file:\n",
        "        \n",
        "        count=0\n",
        "        for line in myfile:\n",
        "          \n",
        "          \n",
        "          count=count+1\n",
        "          if(count==11):\n",
        "            break\n",
        "          # Replace every tab with comma\n",
        "          b=os.path.getsize('marathi/data/wavs/train/'+line[0:21]+'.wav')\n",
        "          \n",
        "          fileContent = re.sub(\"\\t\", \".wav,\"+str(b)+\",\", line)\n",
        "        \n",
        "          # Writing into csv file\n",
        "          csv_file.write(fileContent)\n",
        "\n",
        "    with open('marathi/data/wavs/train/marathi.csv',newline='') as f:\n",
        "      r = csv.reader(f)\n",
        "      data = [line for line in r]\n",
        "    with open('marathi/data/wavs/train/marathi.csv','w',newline='') as f:\n",
        "      w = csv.writer(f)\n",
        "      w.writerow(['wav_filename', 'wav_filesize', 'transcript'])\n",
        "      w.writerows(data)     \n",
        "\n",
        "\n",
        "    with open(\"marathi/data/marathi.tsv\", 'r') as myfile:  \n",
        "      with open(\"marathi/data/wavs/test/marathi_test.csv\", 'w') as csv_file:\n",
        "        \n",
        "        count=0\n",
        "        for line in myfile:\n",
        "          \n",
        "          \n",
        "          count=count+1\n",
        "          if(count<11):\n",
        "            continue\n",
        "          if(count==16):\n",
        "            break  \n",
        "          # Replace every tab with comma\n",
        "          b=os.path.getsize('marathi/data/wavs/test/'+line[0:21]+'.wav')\n",
        "          \n",
        "          fileContent = re.sub(\"\\t\", \".wav,\"+str(b)+\",\", line)\n",
        "        \n",
        "          # Writing into csv file\n",
        "          csv_file.write(fileContent)\n",
        "\n",
        "    with open('marathi/data/wavs/test/marathi_test.csv',newline='') as f:\n",
        "      r = csv.reader(f)\n",
        "      data = [line for line in r]\n",
        "    with open('marathi/data/wavs/test/marathi_test.csv','w',newline='') as f:\n",
        "      w = csv.writer(f)\n",
        "      w.writerow(['wav_filename', 'wav_filesize', 'transcript'])\n",
        "      w.writerows(data)\n",
        "    #fname = 'marathi/data/wavs/train/mrt_01523_00028548203.wav'\n",
        "    \n",
        "    #alpha=\"‡§Ö‡§Ñ‡§Ü‡§á‡§à‡§â‡§ä‡§ã‡§å‡§ç‡§é‡§è‡§ê‡§ë‡§í‡§ì‡§î‡§Ö‡§Ç‡§Ö‡§É‡§æ‡•ã‡§Ç‡•á‡•ç‡•Ä‡•Ç‡•ã‡•å‡•è‡•ê‡§ø‡•Å‡•É‡•Ñ‡•Ü‡•â‡•ä‡•â‡•é‡•à‡§É‡§Å‡•Ö‡§Ä‡§ª‡§º‡•ï‡•ñ‡•ó‡•¢‡•£‡•∞‡•±_‡•ò‡•ô‡•ö‡•õ‡•ú‡•ù‡•û‡•ü‡•†‡•°‡•§‡••‡•¶‡•ß‡•®‡•©‡•™‡•´‡•¨‡•≠‡•Æ‡•Ø‡•≤‡•≥‡•¥‡•µ‡•∂‡•∑‡•∏‡•π‡•∫‡•ª‡•º‡•Ω‡•æ‡•ø?‡§ï‡§ñ‡§ó‡§ò‡§ô‡§ö‡§õ‡§ú‡§ù‡§û‡§ü‡§†‡§°‡§¢‡§£‡§§‡§•‡§¶‡§ß‡§®‡§©‡§™‡§´‡§¨‡§≠‡§Æ‡§Ø‡§∞‡§±‡§≤‡§µ‡§∂‡§∑‡§∏‡§π‡§≥‡§¥‡§ï‡•ç‡§∑‡§ú‡•ç‡§û\"\n",
        "     \n",
        "    \n",
        "    #maybe_download(\"ru.wav\", data_dir, \"https://raw.githubusercontent.com/coqui-ai/STT/main/data/smoke_test/russian_sample_data/ru.wav\")\n",
        "    #maybe_download(\"ru.csv\", data_dir, \"https://raw.githubusercontent.com/coqui-ai/STT/main/data/smoke_test/russian_sample_data/ru.csv\")\n",
        "    #maybe_download(\"alphabet.txt\", \"marathi/data/wavs/train/\", \"https://github.com/divyankiitb/Asr_files/blob/main/alphabet.txt\")\n",
        "    !wget https://github.com/divyankiitb/Asr_files/raw/main/files.zip\n",
        "\n",
        "    !unzip files.zip\n",
        "# Download sample Russian data\n",
        "download_sample_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b46b7227",
      "metadata": {
        "id": "b46b7227"
      },
      "source": [
        "## ‚úÖ Configure the training run\n",
        "\n",
        "Coqui STT comes with a long list of hyperparameters you can tweak. We've set default values, but you can use `initialize_globals_from_args()` to set your own. \n",
        "\n",
        "You must **always** configure the paths to your data, and you must **always** configure your alphabet. For transfer learning, it's good practice to define different `load_checkpoint_dir` and `save_checkpoint_dir` paths so that you keep your new model (Russian STT) separate from the old one (English STT). The parameter `drop_source_layers` allows you to remove layers from the original (aka \"source\") model, and re-initialize them from scratch. If you are fine-tuning to a new alphabet you will have to use _at least_ `drop_source_layers=1` to remove the output layer and add a new output layer which matches your new alphabet.\n",
        "\n",
        "We are fine-tuning a pre-existing model, so `n_hidden` should be the same as the original English model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cff3c5a0",
      "metadata": {
        "id": "cff3c5a0"
      },
      "outputs": [],
      "source": [
        "from coqui_stt_training.util.config import initialize_globals_from_args\n",
        "\n",
        "initialize_globals_from_args(\n",
        "    n_hidden=64,\n",
        "    load_checkpoint_dir=\"english/coqui-yesno-checkpoints\",\n",
        "    save_checkpoint_dir=\"marathi/data/wavs/train/checkpoints\",\n",
        "    drop_source_layers=3,\n",
        "    alphabet_config_path=\"files/alphabet.txt\",\n",
        "    train_files=[\"marathi/data/wavs/train/marathi.csv\"],\n",
        "    dev_files=[\"marathi/data/wavs/train/marathi.csv\"],\n",
        "    epochs=100,\n",
        "    learning_rate=.0001,\n",
        "    dropout_rate=.05,\n",
        "    early_stop=True,\n",
        "    load_cudnn=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "419828c1",
      "metadata": {
        "id": "419828c1"
      },
      "source": [
        "### View all Config settings (*Optional*) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cac6ea3d",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cac6ea3d",
        "outputId": "9c5e04e5-a374-4bd3-b285-257945378a0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"train_files\": [\n",
            "        \"marathi/data/wavs/train/marathi.csv\"\n",
            "    ],\n",
            "    \"dev_files\": [\n",
            "        \"marathi/data/wavs/train/marathi.csv\"\n",
            "    ],\n",
            "    \"test_files\": [],\n",
            "    \"metrics_files\": [],\n",
            "    \"auto_input_dataset\": \"\",\n",
            "    \"vocab_file\": \"\",\n",
            "    \"read_buffer\": 1048576,\n",
            "    \"feature_cache\": \"\",\n",
            "    \"cache_for_epochs\": 0,\n",
            "    \"shuffle_batches\": false,\n",
            "    \"shuffle_start\": 1,\n",
            "    \"shuffle_buffer\": 1000,\n",
            "    \"feature_win_len\": 32,\n",
            "    \"feature_win_step\": 20,\n",
            "    \"audio_sample_rate\": 16000,\n",
            "    \"normalize_sample_rate\": true,\n",
            "    \"augment\": null,\n",
            "    \"epochs\": 100,\n",
            "    \"dropout_rate\": 0.08,\n",
            "    \"dropout_rate2\": 0.08,\n",
            "    \"dropout_rate3\": 0.08,\n",
            "    \"dropout_rate4\": 0.0,\n",
            "    \"dropout_rate5\": 0.0,\n",
            "    \"dropout_rate6\": 0.08,\n",
            "    \"relu_clip\": 20.0,\n",
            "    \"beta1\": 0.9,\n",
            "    \"beta2\": 0.999,\n",
            "    \"epsilon\": 1e-08,\n",
            "    \"learning_rate\": 0.001,\n",
            "    \"train_batch_size\": 1,\n",
            "    \"dev_batch_size\": 1,\n",
            "    \"test_batch_size\": 1,\n",
            "    \"export_batch_size\": 1,\n",
            "    \"inter_op_parallelism_threads\": 0,\n",
            "    \"intra_op_parallelism_threads\": 0,\n",
            "    \"use_allow_growth\": false,\n",
            "    \"load_cudnn\": true,\n",
            "    \"train_cudnn\": false,\n",
            "    \"automatic_mixed_precision\": false,\n",
            "    \"limit_test\": 0,\n",
            "    \"reverse_test\": false,\n",
            "    \"checkpoint_dir\": \"\",\n",
            "    \"load_checkpoint_dir\": \"english/coqui-yesno-checkpoints\",\n",
            "    \"save_checkpoint_dir\": \"marathi/data/wavs/train/checkpoints\",\n",
            "    \"checkpoint_secs\": 600,\n",
            "    \"max_to_keep\": 5,\n",
            "    \"load_train\": \"auto\",\n",
            "    \"load_evaluate\": \"auto\",\n",
            "    \"drop_source_layers\": 1,\n",
            "    \"export_dir\": \"\",\n",
            "    \"remove_export\": false,\n",
            "    \"export_tflite\": true,\n",
            "    \"export_quantize\": true,\n",
            "    \"export_savedmodel\": false,\n",
            "    \"n_steps\": 16,\n",
            "    \"export_zip\": false,\n",
            "    \"export_file_name\": \"output_graph\",\n",
            "    \"export_beam_width\": 500,\n",
            "    \"export_author_id\": \"author\",\n",
            "    \"export_model_name\": \"model\",\n",
            "    \"export_model_version\": \"0.0.1\",\n",
            "    \"export_contact_info\": \"<public contact information of the author. Can be an email address, or a link to a contact form, issue tracker, or discussion forum. Must provide a way to reach the model authors>\",\n",
            "    \"export_license\": \"<SPDX identifier of the license of the exported model. See https://spdx.org/licenses/. If the license does not have an SPDX identifier, use the license name.>\",\n",
            "    \"export_language\": \"<language the model was trained on - IETF BCP 47 language tag including at least language, script and region subtags. E.g. \\\"en-Latn-UK\\\" or \\\"de-Latn-DE\\\" or \\\"cmn-Hans-CN\\\". Include as much info as you can without loss of precision. For example, if a model is trained on Scottish English, include the variant subtag: \\\"en-Latn-GB-Scotland\\\".>\",\n",
            "    \"export_min_stt_version\": \"<minimum Coqui STT version (inclusive) the exported model is compatible with>\",\n",
            "    \"export_max_stt_version\": \"<maximum Coqui STT version (inclusive) the exported model is compatible with>\",\n",
            "    \"export_description\": \"<Freeform description of the model being exported. Markdown accepted. You can also leave this flag unchanged and edit the generated .md file directly. Useful things to describe are demographic and acoustic characteristics of the data used to train the model, any architectural changes, names of public datasets that were used when applicable, hyperparameters used for training, evaluation results on standard benchmark datasets, etc.>\",\n",
            "    \"log_level\": 1,\n",
            "    \"show_progressbar\": true,\n",
            "    \"log_placement\": false,\n",
            "    \"report_count\": 5,\n",
            "    \"summary_dir\": \"marathi/data/wavs/train/checkpoints/summaries\",\n",
            "    \"test_output_file\": \"\",\n",
            "    \"n_hidden\": 64,\n",
            "    \"layer_norm\": false,\n",
            "    \"random_seed\": 4568,\n",
            "    \"early_stop\": true,\n",
            "    \"es_epochs\": 25,\n",
            "    \"es_min_delta\": 0.05,\n",
            "    \"reduce_lr_on_plateau\": false,\n",
            "    \"plateau_epochs\": 10,\n",
            "    \"plateau_reduction\": 0.1,\n",
            "    \"force_initialize_learning_rate\": false,\n",
            "    \"bytes_output_mode\": false,\n",
            "    \"alphabet_config_path\": \"files/alphabet.txt\",\n",
            "    \"scorer_path\": \"\",\n",
            "    \"beam_width\": 1024,\n",
            "    \"lm_alpha\": 0.931289039105002,\n",
            "    \"lm_beta\": 1.1834137581510284,\n",
            "    \"cutoff_prob\": 1.0,\n",
            "    \"cutoff_top_n\": 300,\n",
            "    \"one_shot_infer\": null,\n",
            "    \"lm_alpha_max\": 5,\n",
            "    \"lm_beta_max\": 5,\n",
            "    \"n_trials\": 2400\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from coqui_stt_training.util.config import Config\n",
        "\n",
        "print(Config.to_json())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8e700d1",
      "metadata": {
        "id": "c8e700d1"
      },
      "source": [
        "## ‚úÖ Train a new Russian model\n",
        "\n",
        "Let's kick off a training run üöÄüöÄüöÄ (using the configure you set above).\n",
        "\n",
        "This notebook should work on either a GPU or a CPU. However, in case you're running this on _multiple_ GPUs we want to only use one, because the sample dataset (one audio file) is too small to split across multiple GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8aab2195",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aab2195",
        "outputId": "c2a0de55-350b-4de8-9371-3da3e432f004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I Performing dummy training to check for memory problems.\n",
            "I If the following process crashes, you likely have batch sizes that are too big for your available system memory (or GPU memory).\n",
            "I Loading best validating checkpoint from english/coqui-yesno-checkpoints/best_dev-1909\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam_1\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam_1\n",
            "I Loading variable from checkpoint: beta1_power\n",
            "I Loading variable from checkpoint: beta2_power\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam_1\n",
            "I Loading variable from checkpoint: learning_rate\n",
            "I Initializing variable: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam\n",
            "I Initializing variable: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam_1\n",
            "I Initializing variable: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam\n",
            "I Initializing variable: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam_1\n",
            "I Initializing variable: layer_6/bias\n",
            "I Initializing variable: layer_6/bias/Adam\n",
            "I Initializing variable: layer_6/bias/Adam_1\n",
            "I Initializing variable: layer_6/weights\n",
            "I Initializing variable: layer_6/weights/Adam\n",
            "I Initializing variable: layer_6/weights/Adam_1\n",
            "I STARTING Optimization\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:02 | Steps: 3 | Loss: 1353.044312    \n",
            "Epoch 0 | Validation | Elapsed Time: 0:00:01 | Steps: 3 | Loss: 848.595723 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "I FINISHED optimization in 0:00:03.759561\n",
            "I Dummy run finished without problems, now starting real training process.\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam_1\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam_1\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam\n",
            "W CUDNN variable not found: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam\n",
            "I STARTING Optimization\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 773.837885    \n",
            "Epoch 0 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 492.890631 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 492.890631 to: marathi/data/wavs/train/checkpoints/best_dev-1919\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 438.370374    \n",
            "Epoch 1 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 282.105458 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 282.105458 to: marathi/data/wavs/train/checkpoints/best_dev-1929\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 256.656482    \n",
            "Epoch 2 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 252.739124 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 252.739124 to: marathi/data/wavs/train/checkpoints/best_dev-1939\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 257.959956    \n",
            "Epoch 3 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 238.904058 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 238.904058 to: marathi/data/wavs/train/checkpoints/best_dev-1949\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 241.886932    \n",
            "Epoch 4 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 237.506796 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 237.506796 to: marathi/data/wavs/train/checkpoints/best_dev-1959\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 244.165461    \n",
            "Epoch 5 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 234.127661 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 234.127661 to: marathi/data/wavs/train/checkpoints/best_dev-1969\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 236.704030    \n",
            "Epoch 6 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 238.097250 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 242.346596    \n",
            "Epoch 7 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 229.899503 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 229.899503 to: marathi/data/wavs/train/checkpoints/best_dev-1989\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 8 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 233.519689    \n",
            "Epoch 8 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 229.304625 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 229.304625 to: marathi/data/wavs/train/checkpoints/best_dev-1999\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 9 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 232.313181    \n",
            "Epoch 9 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 229.787090 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 10 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 231.447595   \n",
            "Epoch 10 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 226.549057 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 226.549057 to: marathi/data/wavs/train/checkpoints/best_dev-2019\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 11 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 228.047874   \n",
            "Epoch 11 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 224.698430 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 224.698430 to: marathi/data/wavs/train/checkpoints/best_dev-2029\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 12 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 226.075713   \n",
            "Epoch 12 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 223.816787 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 223.816787 to: marathi/data/wavs/train/checkpoints/best_dev-2039\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 13 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 224.817855   \n",
            "Epoch 13 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 223.753731 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 223.753731 to: marathi/data/wavs/train/checkpoints/best_dev-2049\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 14 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 221.152320   \n",
            "Epoch 14 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 222.385773 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 222.385773 to: marathi/data/wavs/train/checkpoints/best_dev-2059\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 15 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 224.327386   \n",
            "Epoch 15 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 219.190811 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 219.190811 to: marathi/data/wavs/train/checkpoints/best_dev-2069\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 16 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 218.601900   \n",
            "Epoch 16 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 226.513876 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 17 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 220.765402   \n",
            "Epoch 17 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 215.575990 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 215.575990 to: marathi/data/wavs/train/checkpoints/best_dev-2089\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 18 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 217.686241   \n",
            "Epoch 18 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 219.959338 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 19 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 215.858025   \n",
            "Epoch 19 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 215.251106 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 215.251106 to: marathi/data/wavs/train/checkpoints/best_dev-2109\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 20 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 214.354671   \n",
            "Epoch 20 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 214.495920 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 214.495920 to: marathi/data/wavs/train/checkpoints/best_dev-2119\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 21 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 211.468538   \n",
            "Epoch 21 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 212.796954 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 212.796954 to: marathi/data/wavs/train/checkpoints/best_dev-2129\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 22 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 214.020684   \n",
            "Epoch 22 | Validation | Elapsed Time: 0:00:03 | Steps: 10 | Loss: 211.580521 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 211.580521 to: marathi/data/wavs/train/checkpoints/best_dev-2139\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 23 |   Training | Elapsed Time: 0:00:03 | Steps: 10 | Loss: 212.975715   \n",
            "Epoch 23 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 212.138146 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 24 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 210.039246   \n",
            "Epoch 24 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 205.292414 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 205.292414 to: marathi/data/wavs/train/checkpoints/best_dev-2159\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 25 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 209.481897   \n",
            "Epoch 25 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 203.969991 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 203.969991 to: marathi/data/wavs/train/checkpoints/best_dev-2169\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 26 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 209.335999   \n",
            "Epoch 26 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 203.499786 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 203.499786 to: marathi/data/wavs/train/checkpoints/best_dev-2179\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 27 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 206.770744   \n",
            "Epoch 27 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 203.901946 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 28 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 206.934892   \n",
            "Epoch 28 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 202.508022 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 202.508022 to: marathi/data/wavs/train/checkpoints/best_dev-2199\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 29 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 202.359756   \n",
            "Epoch 29 | Validation | Elapsed Time: 0:00:03 | Steps: 10 | Loss: 202.080917 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 202.080917 to: marathi/data/wavs/train/checkpoints/best_dev-2209\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 30 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 201.956631   \n",
            "Epoch 30 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 201.785784 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 201.785784 to: marathi/data/wavs/train/checkpoints/best_dev-2219\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 31 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 202.796925   \n",
            "Epoch 31 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 201.231884 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 201.231884 to: marathi/data/wavs/train/checkpoints/best_dev-2229\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 32 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 203.260405   \n",
            "Epoch 32 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 199.927137 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 199.927137 to: marathi/data/wavs/train/checkpoints/best_dev-2239\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 33 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 198.829482   \n",
            "Epoch 33 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 199.876740 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 199.876740 to: marathi/data/wavs/train/checkpoints/best_dev-2249\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 34 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 197.338120   \n",
            "Epoch 34 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 198.201838 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 198.201838 to: marathi/data/wavs/train/checkpoints/best_dev-2259\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 35 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 197.332048   \n",
            "Epoch 35 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 199.021425 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 36 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 196.991963   \n",
            "Epoch 36 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 197.641769 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 197.641769 to: marathi/data/wavs/train/checkpoints/best_dev-2279\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 37 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 195.033538   \n",
            "Epoch 37 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 198.042351 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 38 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 193.759078   \n",
            "Epoch 38 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 197.645577 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 39 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 191.278973   \n",
            "Epoch 39 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 198.170553 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 40 |   Training | Elapsed Time: 0:00:03 | Steps: 10 | Loss: 193.011478   \n",
            "Epoch 40 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 197.207695 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 197.207695 to: marathi/data/wavs/train/checkpoints/best_dev-2319\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 41 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 192.701902   \n",
            "Epoch 41 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 197.601676 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 42 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 198.328916   \n",
            "Epoch 42 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 197.461747 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 43 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 201.988909   \n",
            "Epoch 43 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 193.360380 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 193.360380 to: marathi/data/wavs/train/checkpoints/best_dev-2349\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 44 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 192.146288   \n",
            "Epoch 44 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 194.390810 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 45 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 194.065948   \n",
            "Epoch 45 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 193.046529 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 193.046529 to: marathi/data/wavs/train/checkpoints/best_dev-2369\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 46 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 192.050468   \n",
            "Epoch 46 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 191.203621 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 191.203621 to: marathi/data/wavs/train/checkpoints/best_dev-2379\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 47 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 191.763266   \n",
            "Epoch 47 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 192.110528 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 48 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 188.880358   \n",
            "Epoch 48 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 191.665711 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 49 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 186.988155   \n",
            "Epoch 49 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 189.848515 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 189.848515 to: marathi/data/wavs/train/checkpoints/best_dev-2409\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 50 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 188.132819   \n",
            "Epoch 50 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 188.997672 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 188.997672 to: marathi/data/wavs/train/checkpoints/best_dev-2419\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 51 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 184.919725   \n",
            "Epoch 51 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 187.809352 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 187.809352 to: marathi/data/wavs/train/checkpoints/best_dev-2429\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 52 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 186.587550   \n",
            "Epoch 52 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 188.204514 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 53 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 186.288412   \n",
            "Epoch 53 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 186.838362 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 186.838362 to: marathi/data/wavs/train/checkpoints/best_dev-2449\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 54 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 184.036242   \n",
            "Epoch 54 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 186.864707 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 55 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 190.516337   \n",
            "Epoch 55 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 185.486737 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 185.486737 to: marathi/data/wavs/train/checkpoints/best_dev-2469\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 56 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 183.084838   \n",
            "Epoch 56 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 185.151015 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 185.151015 to: marathi/data/wavs/train/checkpoints/best_dev-2479\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 57 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 182.622972   \n",
            "Epoch 57 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 186.825649 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 58 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 183.335050   \n",
            "Epoch 58 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 185.476559 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 59 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 181.470490   \n",
            "Epoch 59 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 185.120537 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 185.120537 to: marathi/data/wavs/train/checkpoints/best_dev-2509\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 60 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 182.320478   \n",
            "Epoch 60 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 184.579205 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 184.579205 to: marathi/data/wavs/train/checkpoints/best_dev-2519\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 61 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 182.717516   \n",
            "Epoch 61 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 182.867889 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 182.867889 to: marathi/data/wavs/train/checkpoints/best_dev-2529\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 62 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 180.589330   \n",
            "Epoch 62 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 182.598564 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 182.598564 to: marathi/data/wavs/train/checkpoints/best_dev-2539\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 63 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 183.087717   \n",
            "Epoch 63 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 180.067793 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 180.067793 to: marathi/data/wavs/train/checkpoints/best_dev-2549\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 64 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 182.490871   \n",
            "Epoch 64 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 185.542297 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 65 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 178.629015   \n",
            "Epoch 65 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 183.381721 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 66 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 179.602464   \n",
            "Epoch 66 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 182.269673 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 67 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 177.002316   \n",
            "Epoch 67 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 180.681531 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 68 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 176.432073   \n",
            "Epoch 68 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 180.711787 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 69 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 177.591228   \n",
            "Epoch 69 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 180.382635 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 70 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 177.859045   \n",
            "Epoch 70 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 178.869353 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 178.869353 to: marathi/data/wavs/train/checkpoints/best_dev-2619\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 71 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 179.420709   \n",
            "Epoch 71 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 177.907193 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 177.907193 to: marathi/data/wavs/train/checkpoints/best_dev-2629\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 72 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 175.815755   \n",
            "Epoch 72 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 179.116920 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 73 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 174.626010   \n",
            "Epoch 73 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 177.929757 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 74 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 175.267712   \n",
            "Epoch 74 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 177.714375 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 177.714375 to: marathi/data/wavs/train/checkpoints/best_dev-2659\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 75 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 176.875255   \n",
            "Epoch 75 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 179.971092 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 76 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 180.239918   \n",
            "Epoch 76 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 177.998447 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 77 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 175.918127   \n",
            "Epoch 77 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 178.086401 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 78 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 176.279462   \n",
            "Epoch 78 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 176.805373 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 176.805373 to: marathi/data/wavs/train/checkpoints/best_dev-2699\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 79 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 175.945107   \n",
            "Epoch 79 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 175.781556 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 175.781556 to: marathi/data/wavs/train/checkpoints/best_dev-2709\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 80 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 172.919462   \n",
            "Epoch 80 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 175.168298 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 175.168298 to: marathi/data/wavs/train/checkpoints/best_dev-2719\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 81 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 173.264101   \n",
            "Epoch 81 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 174.899824 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 174.899824 to: marathi/data/wavs/train/checkpoints/best_dev-2729\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 82 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 175.114389   \n",
            "Epoch 82 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 174.632938 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 174.632938 to: marathi/data/wavs/train/checkpoints/best_dev-2739\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 83 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 173.094354   \n",
            "Epoch 83 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 173.911221 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 173.911221 to: marathi/data/wavs/train/checkpoints/best_dev-2749\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 84 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 172.124015   \n",
            "Epoch 84 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 171.965099 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 171.965099 to: marathi/data/wavs/train/checkpoints/best_dev-2759\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 85 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 171.905081   \n",
            "Epoch 85 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 171.839729 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 171.839729 to: marathi/data/wavs/train/checkpoints/best_dev-2769\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 86 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 170.988177   \n",
            "Epoch 86 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 172.438999 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 87 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 170.818755   \n",
            "Epoch 87 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 171.430071 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 171.430071 to: marathi/data/wavs/train/checkpoints/best_dev-2789\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 88 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 169.757618   \n",
            "Epoch 88 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 172.152440 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 89 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 170.709665   \n",
            "Epoch 89 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 171.405879 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 171.405879 to: marathi/data/wavs/train/checkpoints/best_dev-2809\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 90 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 168.612643   \n",
            "Epoch 90 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 171.957686 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 91 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 168.570182   \n",
            "Epoch 91 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 170.265075 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 170.265075 to: marathi/data/wavs/train/checkpoints/best_dev-2829\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 92 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 169.568877   \n",
            "Epoch 92 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 170.376610 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 93 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 171.525269   \n",
            "Epoch 93 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 173.716956 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 94 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 176.593713   \n",
            "Epoch 94 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 172.737960 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 95 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 174.330405   \n",
            "Epoch 95 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 171.526549 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 96 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 170.846997   \n",
            "Epoch 96 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 170.412573 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 97 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 170.559985   \n",
            "Epoch 97 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 171.921359 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 98 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 174.772492   \n",
            "Epoch 98 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 168.100548 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 168.100548 to: marathi/data/wavs/train/checkpoints/best_dev-2899\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 99 |   Training | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 168.996137   \n",
            "Epoch 99 | Validation | Elapsed Time: 0:00:02 | Steps: 10 | Loss: 167.882797 | Dataset: marathi/data/wavs/train/marathi.csv\n",
            "I Saved new best validating model with loss 167.882797 to: marathi/data/wavs/train/checkpoints/best_dev-2909\n",
            "--------------------------------------------------------------------------------\n",
            "I FINISHED optimization in 0:08:52.374379\n"
          ]
        }
      ],
      "source": [
        "from coqui_stt_training.train import train\n",
        "\n",
        "# use maximum one GPU\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c87ba61",
      "metadata": {
        "id": "3c87ba61"
      },
      "source": [
        "## ‚úÖ Configure the testing run\n",
        "\n",
        "Let's add the path to our testing data and update `load_checkpoint_dir` to our new model checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2be7beb5",
      "metadata": {
        "id": "2be7beb5"
      },
      "outputs": [],
      "source": [
        "from coqui_stt_training.util.config import Config\n",
        "\n",
        "Config.test_files=[\"marathi/data/wavs/test/marathi_test.csv\"]\n",
        "Config.load_checkpoint_dir=\"marathi/data/wavs/train/checkpoints\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6a5c971",
      "metadata": {
        "id": "c6a5c971"
      },
      "source": [
        "## ‚úÖ Test the new Russian model\n",
        "\n",
        "We made it! üôå\n",
        "\n",
        "Let's kick off the testing run, which displays performance metrics.\n",
        "\n",
        "We're committing the cardinal sin of ML üòà (aka - testing on our training data) so you don't want to deploy this model into production. In this notebook we're focusing on the workflow itself, so it's forgivable üòá\n",
        "\n",
        "You can see from the test output that our tiny model has overfit to the data, and basically memorized this one sentence.\n",
        "\n",
        "When you start training your own models, make sure your testing data doesn't include your training data üòÖ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6222dc69",
      "metadata": {
        "id": "6222dc69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91dfb6d6-e2e7-4a35-d236-f5ecdd45eb11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I Loading best validating checkpoint from marathi/data/wavs/train/checkpoints/best_dev-2849\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "Testing model on marathi/data/wavs/test/marathi_test.csv\n",
            "Test epoch | Steps: 5 | Elapsed Time: 0:01:14                                  \n",
            "Test on marathi/data/wavs/test/marathi_test.csv - WER: 1.000000, CER: 0.829032, loss: 659.688416\n",
            "--------------------------------------------------------------------------------\n",
            "Best WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.821429, loss: 882.066162\n",
            " - wav: file://marathi/data/wavs/test/mrt_09697_02139010528.wav\n",
            " - src: \"‡§Ü‡§§‡§æ ‡§∏‡§æ‡§∞‡§∏‡•ç‡§µ‡§§ ‡§¨‡§Å‡§ï‡•á‡§ö‡•Ä ‡§∏‡§æ‡§§‡§µ‡•Ä ‡§∂‡§æ‡§ñ‡§æ ‡§´‡•Ö‡§∂‡§® ‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä‡§ü‡§ú‡§µ‡§≥ ‡§â‡§ò‡§°‡§§‡•á ‡§Ü‡§π‡•á ‡§™‡§∞‡§µ‡§æ ‡§§‡§ø‡§ö‡§æ ‡§â‡§¶‡•ç‡§ò‡§æ‡§ü‡§® ‡§∏‡§Æ‡§æ‡§∞‡§Ç‡§≠ ‡§Ü‡§π‡•á\"\n",
            " - res: \"‡§ï‡§ï  ‡§æ ‡§æ ‡•Ä ‡•Ä ‡§ï ‡§∞ ‡§∞ ‡§æ‡§∂‡•á\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.902778, loss: 497.626617\n",
            " - wav: file://marathi/data/wavs/test/mrt_03397_02119986802.wav\n",
            " - src: \"‡§Ö‡§®‡•Å‡§ú‡§æ‡§ö‡§æ ‡§ú‡§®‡•ç‡§Æ ‡§è‡§ï‡•ã‡§£‡•Ä‡§∏‡§∂‡•á ‡§Ö‡§†‡•ç‡§†‡•ç‡§Ø‡§æ‡§£‡•ç‡§£‡§µ ‡§∏‡§æ‡§≤‡§æ‡§§‡•Ä‡§≤ ‡§ú‡•Å‡§≤‡•à ‡§Ø‡§æ ‡§Æ‡§π‡§ø‡§®‡•ç‡§Ø‡§æ‡§§ ‡§ß‡•Å‡§≥‡•á ‡§á‡§•‡•á ‡§ù‡§æ‡§≤‡§æ\"\n",
            " - res: \" ‡§æ   ‡§Æ‡§æ\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.200000, CER: 0.718750, loss: 115.777122\n",
            " - wav: file://marathi/data/wavs/test/mrt_04310_02117569342.wav\n",
            " - src: \"‡§ï‡§∞‡§ø‡§®‡§æ‡§ö‡§æ ‡§∏‡§æ‡§Ç‡§≠‡§æ‡§≥ ‡§§‡§ø‡§ö‡•ç‡§Ø‡§æ ‡§Ü‡§à‡§®‡•á‡§ö ‡§ï‡•á‡§≤‡§æ\"\n",
            " - res: \" ‡§æ ‡§æ ‡§æ  ‡•Ä ‡§ï ‡•ç‡§æ\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.272727, CER: 0.684211, loss: 1451.234619\n",
            " - wav: file://marathi/data/wavs/test/mrt_03398_02142956376.wav\n",
            " - src: \"‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§µ‡§æ‡§ü ‡§™‡§π‡§æ‡§µ‡•Ä ‡§≤‡§æ‡§ó‡§≤‡•Ä ‡§ï‡§æ ? ‡§ï‡•ç‡§µ‡§ö‡§ø‡§§ ‡§ï‡§ß‡•Ä ‡§Æ‡§æ‡§ù‡§æ ‡§µ‡•á‡§ó ‡§Æ‡§Ç‡§¶‡§æ‡§µ‡§§‡•ã\"\n",
            " - res: \" ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡•Ä ‡•Ä ‡§ï ‡§ï‡§∂‡§æ‡§∂‡§æ‡§∂‡§æ‡§∂‡§æ‡§∂‡§æ ‡§æ ‡§æ ‡§æ ‡§æ \"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 3.083333, CER: 0.938462, loss: 351.737488\n",
            " - wav: file://marathi/data/wavs/test/mrt_03398_02145517609.wav\n",
            " - src: \"‡§Ü‡§ú ‡§ó‡•Å‡§ó‡§≤ ‡§¶‡•ã‡§® ‡§™‡•Ç‡§∞‡•ç‡§£‡§æ‡§Ç‡§ï ‡§§‡•Ä‡§® ‡§ü‡§ï‡•ç‡§ï‡•á ‡§ò‡§∏‡§∞‡•Ç‡§® ‡§∏‡§æ‡§§‡§∂‡•á ‡§™‡§®‡•ç‡§®‡§æ‡§∏ ‡§°‡•â‡§≤‡§∞‡§µ‡§∞ ‡§¨‡§Ç‡§¶ ‡§ù‡§æ‡§≤‡§Ç\"\n",
            " - res: \" ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡•Ä ‡•Ä ‡§ï ‡§ï ‡§∞ ‡§∞ ‡§æ ‡§æ ‡•ç ‡§∞ ‡§¶‡•ç‡§∂ ‡§∂ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ \"\n",
            "--------------------------------------------------------------------------------\n",
            "Median WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.821429, loss: 882.066162\n",
            " - wav: file://marathi/data/wavs/test/mrt_09697_02139010528.wav\n",
            " - src: \"‡§Ü‡§§‡§æ ‡§∏‡§æ‡§∞‡§∏‡•ç‡§µ‡§§ ‡§¨‡§Å‡§ï‡•á‡§ö‡•Ä ‡§∏‡§æ‡§§‡§µ‡•Ä ‡§∂‡§æ‡§ñ‡§æ ‡§´‡•Ö‡§∂‡§® ‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä‡§ü‡§ú‡§µ‡§≥ ‡§â‡§ò‡§°‡§§‡•á ‡§Ü‡§π‡•á ‡§™‡§∞‡§µ‡§æ ‡§§‡§ø‡§ö‡§æ ‡§â‡§¶‡•ç‡§ò‡§æ‡§ü‡§® ‡§∏‡§Æ‡§æ‡§∞‡§Ç‡§≠ ‡§Ü‡§π‡•á\"\n",
            " - res: \"‡§ï‡§ï  ‡§æ ‡§æ ‡•Ä ‡•Ä ‡§ï ‡§∞ ‡§∞ ‡§æ‡§∂‡•á\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.902778, loss: 497.626617\n",
            " - wav: file://marathi/data/wavs/test/mrt_03397_02119986802.wav\n",
            " - src: \"‡§Ö‡§®‡•Å‡§ú‡§æ‡§ö‡§æ ‡§ú‡§®‡•ç‡§Æ ‡§è‡§ï‡•ã‡§£‡•Ä‡§∏‡§∂‡•á ‡§Ö‡§†‡•ç‡§†‡•ç‡§Ø‡§æ‡§£‡•ç‡§£‡§µ ‡§∏‡§æ‡§≤‡§æ‡§§‡•Ä‡§≤ ‡§ú‡•Å‡§≤‡•à ‡§Ø‡§æ ‡§Æ‡§π‡§ø‡§®‡•ç‡§Ø‡§æ‡§§ ‡§ß‡•Å‡§≥‡•á ‡§á‡§•‡•á ‡§ù‡§æ‡§≤‡§æ\"\n",
            " - res: \" ‡§æ   ‡§Æ‡§æ\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.200000, CER: 0.718750, loss: 115.777122\n",
            " - wav: file://marathi/data/wavs/test/mrt_04310_02117569342.wav\n",
            " - src: \"‡§ï‡§∞‡§ø‡§®‡§æ‡§ö‡§æ ‡§∏‡§æ‡§Ç‡§≠‡§æ‡§≥ ‡§§‡§ø‡§ö‡•ç‡§Ø‡§æ ‡§Ü‡§à‡§®‡•á‡§ö ‡§ï‡•á‡§≤‡§æ\"\n",
            " - res: \" ‡§æ ‡§æ ‡§æ  ‡•Ä ‡§ï ‡•ç‡§æ\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.272727, CER: 0.684211, loss: 1451.234619\n",
            " - wav: file://marathi/data/wavs/test/mrt_03398_02142956376.wav\n",
            " - src: \"‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§µ‡§æ‡§ü ‡§™‡§π‡§æ‡§µ‡•Ä ‡§≤‡§æ‡§ó‡§≤‡•Ä ‡§ï‡§æ ? ‡§ï‡•ç‡§µ‡§ö‡§ø‡§§ ‡§ï‡§ß‡•Ä ‡§Æ‡§æ‡§ù‡§æ ‡§µ‡•á‡§ó ‡§Æ‡§Ç‡§¶‡§æ‡§µ‡§§‡•ã\"\n",
            " - res: \" ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡•Ä ‡•Ä ‡§ï ‡§ï‡§∂‡§æ‡§∂‡§æ‡§∂‡§æ‡§∂‡§æ‡§∂‡§æ ‡§æ ‡§æ ‡§æ ‡§æ \"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 3.083333, CER: 0.938462, loss: 351.737488\n",
            " - wav: file://marathi/data/wavs/test/mrt_03398_02145517609.wav\n",
            " - src: \"‡§Ü‡§ú ‡§ó‡•Å‡§ó‡§≤ ‡§¶‡•ã‡§® ‡§™‡•Ç‡§∞‡•ç‡§£‡§æ‡§Ç‡§ï ‡§§‡•Ä‡§® ‡§ü‡§ï‡•ç‡§ï‡•á ‡§ò‡§∏‡§∞‡•Ç‡§® ‡§∏‡§æ‡§§‡§∂‡•á ‡§™‡§®‡•ç‡§®‡§æ‡§∏ ‡§°‡•â‡§≤‡§∞‡§µ‡§∞ ‡§¨‡§Ç‡§¶ ‡§ù‡§æ‡§≤‡§Ç\"\n",
            " - res: \" ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡•Ä ‡•Ä ‡§ï ‡§ï ‡§∞ ‡§∞ ‡§æ ‡§æ ‡•ç ‡§∞ ‡§¶‡•ç‡§∂ ‡§∂ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ \"\n",
            "--------------------------------------------------------------------------------\n",
            "Worst WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.821429, loss: 882.066162\n",
            " - wav: file://marathi/data/wavs/test/mrt_09697_02139010528.wav\n",
            " - src: \"‡§Ü‡§§‡§æ ‡§∏‡§æ‡§∞‡§∏‡•ç‡§µ‡§§ ‡§¨‡§Å‡§ï‡•á‡§ö‡•Ä ‡§∏‡§æ‡§§‡§µ‡•Ä ‡§∂‡§æ‡§ñ‡§æ ‡§´‡•Ö‡§∂‡§® ‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä‡§ü‡§ú‡§µ‡§≥ ‡§â‡§ò‡§°‡§§‡•á ‡§Ü‡§π‡•á ‡§™‡§∞‡§µ‡§æ ‡§§‡§ø‡§ö‡§æ ‡§â‡§¶‡•ç‡§ò‡§æ‡§ü‡§® ‡§∏‡§Æ‡§æ‡§∞‡§Ç‡§≠ ‡§Ü‡§π‡•á\"\n",
            " - res: \"‡§ï‡§ï  ‡§æ ‡§æ ‡•Ä ‡•Ä ‡§ï ‡§∞ ‡§∞ ‡§æ‡§∂‡•á\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.902778, loss: 497.626617\n",
            " - wav: file://marathi/data/wavs/test/mrt_03397_02119986802.wav\n",
            " - src: \"‡§Ö‡§®‡•Å‡§ú‡§æ‡§ö‡§æ ‡§ú‡§®‡•ç‡§Æ ‡§è‡§ï‡•ã‡§£‡•Ä‡§∏‡§∂‡•á ‡§Ö‡§†‡•ç‡§†‡•ç‡§Ø‡§æ‡§£‡•ç‡§£‡§µ ‡§∏‡§æ‡§≤‡§æ‡§§‡•Ä‡§≤ ‡§ú‡•Å‡§≤‡•à ‡§Ø‡§æ ‡§Æ‡§π‡§ø‡§®‡•ç‡§Ø‡§æ‡§§ ‡§ß‡•Å‡§≥‡•á ‡§á‡§•‡•á ‡§ù‡§æ‡§≤‡§æ\"\n",
            " - res: \" ‡§æ   ‡§Æ‡§æ\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.200000, CER: 0.718750, loss: 115.777122\n",
            " - wav: file://marathi/data/wavs/test/mrt_04310_02117569342.wav\n",
            " - src: \"‡§ï‡§∞‡§ø‡§®‡§æ‡§ö‡§æ ‡§∏‡§æ‡§Ç‡§≠‡§æ‡§≥ ‡§§‡§ø‡§ö‡•ç‡§Ø‡§æ ‡§Ü‡§à‡§®‡•á‡§ö ‡§ï‡•á‡§≤‡§æ\"\n",
            " - res: \" ‡§æ ‡§æ ‡§æ  ‡•Ä ‡§ï ‡•ç‡§æ\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.272727, CER: 0.684211, loss: 1451.234619\n",
            " - wav: file://marathi/data/wavs/test/mrt_03398_02142956376.wav\n",
            " - src: \"‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§µ‡§æ‡§ü ‡§™‡§π‡§æ‡§µ‡•Ä ‡§≤‡§æ‡§ó‡§≤‡•Ä ‡§ï‡§æ ? ‡§ï‡•ç‡§µ‡§ö‡§ø‡§§ ‡§ï‡§ß‡•Ä ‡§Æ‡§æ‡§ù‡§æ ‡§µ‡•á‡§ó ‡§Æ‡§Ç‡§¶‡§æ‡§µ‡§§‡•ã\"\n",
            " - res: \" ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡•Ä ‡•Ä ‡§ï ‡§ï‡§∂‡§æ‡§∂‡§æ‡§∂‡§æ‡§∂‡§æ‡§∂‡§æ ‡§æ ‡§æ ‡§æ ‡§æ \"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 3.083333, CER: 0.938462, loss: 351.737488\n",
            " - wav: file://marathi/data/wavs/test/mrt_03398_02145517609.wav\n",
            " - src: \"‡§Ü‡§ú ‡§ó‡•Å‡§ó‡§≤ ‡§¶‡•ã‡§® ‡§™‡•Ç‡§∞‡•ç‡§£‡§æ‡§Ç‡§ï ‡§§‡•Ä‡§® ‡§ü‡§ï‡•ç‡§ï‡•á ‡§ò‡§∏‡§∞‡•Ç‡§® ‡§∏‡§æ‡§§‡§∂‡•á ‡§™‡§®‡•ç‡§®‡§æ‡§∏ ‡§°‡•â‡§≤‡§∞‡§µ‡§∞ ‡§¨‡§Ç‡§¶ ‡§ù‡§æ‡§≤‡§Ç\"\n",
            " - res: \" ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡•Ä ‡•Ä ‡§ï ‡§ï ‡§∞ ‡§∞ ‡§æ ‡§æ ‡•ç ‡§∞ ‡§¶‡•ç‡§∂ ‡§∂ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ ‡§æ \"\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from coqui_stt_training.evaluate import test\n",
        "\n",
        "test()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "final_of easy_transfer_learning.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}